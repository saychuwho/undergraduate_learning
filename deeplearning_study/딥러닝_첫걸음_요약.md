# 딥러닝 첫걸음

## 1장 머신러닝

- 인공지능과 머신러닝, 딥러닝은 구별해서 사용해야 합니다. 세 단어의 관계를 정리하면 다음과 같습니다. "딥러닝은 머신러닝의 일종이고 머신러닝은 인공지능의 일종이다."
- 머신러닝은 "학습 데이터"로부터 "모델"을 찾아내는 귀납적 기법으로 영상 인식과 음성 인식 및 자연어 처리 등의 문제를 푸는 데 효과적입니다.
- 머신러닝의 성패는 일반화를 얼마나 잘 달성하느냐에 달려 있습니다. 학습 데이터와 실제 입력 데이터의 차이로 인한 성능 저하를 막기 위해서는 편향되지 않은 학습 데이터를 충분히 확보해야 합니다.
- 과적합은 모델이 학습 데이터에 지나치게 최적화되어 학습 데이터에서는 성능이 좋지만, 실제 입력 데이터에서는 성능이 크게 떨어지는 현상을 의미합니다. 과적합은 머신러닝 모델의 일반화 성능을 떨어뜨리는 주요 원인 중의 하나입니다.
- 과적합 문제를 해결하기 위한 대표적인 기법으로는 정칙화와 검증이 있습니다. 정칙화는 최대한 단순한 모델을 유도하려는 수치해석적인 기법입니다. 검증은 학습 과정에서 과적합 여부를 미리 알아내 이에 대한 조치를 취하려는 목적으로 고안된 기법입니다. 검증의 변형된 형태로 교차 검증이 있습니다.
- 머신러닝은 학습 방법에 따라 지도학습, 비지도학습, 강화학습으로 나눌 수 있습니다. 학습 종류별로 학습 데이터는 다음과 같은 형태로 주어집니다. 
	- 지도학습 : 입력, 정답이 학습데이터
	- 비지도학습 : 입력이 학습데이터
    - 강화학습 : 입력, 출력, 출력에 대한 점수 가 학습데이터
- 지도학습은 모델의 용도에 따라 크게 '분류'와 '회귀' 문제로 나눠집니다. 분류는 입력 데이터가 어느 범주에 속하는지를 알아내는 문제입니다. 분류 문제에서 학습 데이터의 정답은 '범주'로 주어집니다. 회귀 문제는 값을 예측하는 문제입니다. 학습 데이터의 정답은 '값'으로 주어집니다. 

## 2장 신경망

- 신경망은 뇌의 신경세포를 모사해 만든 노드들의 네트워크입니다. 노드느 입력 신호들의 가중합을 구하고, 이 값을 활성함수에 입력해 나온 값을 외부로 출력합니다.
- 대부분의 신경망은 노드들을 계층적으로 배치한 구조로 되어있습니다. 계층 구조로 된 신경망에서 신호는 입력층으로 들어가 은닉층을 거쳐 출력층의 순서로 처리됩니다.
- 원칙적으로 은닉층의 노드에서는 활성함수로 선형함수를 사용하면 안됩니다. 은닉층을 추가한 효과가 사라지기 때문입니다. 출력층의 노드에서는 회귀 문제와 같이 선형함수를 사용해야 하는 경우가 있습니다.
- 신경망에서 지도학습은 신경망의 출력과 주어진 정답의 차이를 줄이도록 연결 가중치를 변경하는 과정을 의미합니다.
- 주어진 학습 데이터에 맞춰 신경망의 가중치를 변경하는 방법을 '학습 규칙'이라고 부릅니다.
- 신경망의 지도학습에서 오차를 계산하는 방법은 크게 세 가지가 있습니다. Stochastic 경사 하강법, 배치, 미니배치
- 델타 규칙은 단층 신경망의 대표적인 학습 규칙입니다. 델타 규칙의 수식은 활성함후에 따라 조금씩 달라집니다.
    $$\delta_i = \psi'(v_i)e_i$$ 
    $$ w_{ij} \larr w_{ij} + \alpha\delta_ix_i  $$
- 델타 규칙은 반복 작업을 통해 정답을 단계적으로 찾아가는 기법이기 때문에 신경망의 오차가 충분히 줄어들 때까지 학습 데이터를 반복해서 재학습시켜야 합니다.
- 단층 신경망은 특정 유형의 문제에만 적용할 수 있습니다. 대부분의 실제 문제는 이런 유형이 아니여서 단층 신경망으로 해결할 수 있는 문제는 매우 제한적입니다. 다층 신경망은 이러한 단층 신경망의 근본적인 한계를 극복하기 위해 개발되었습니다

## 3장 다층 신경망의 학습

- 델타 규칙으로는 다층 신경망을 학습시킬 수 없습니다. 다층 신경망은 역전파 알고리즘으로 학습시켜야 합니다. 역전파 알고리즘은 딥러닝에서도 사용되는 학습 규칙입니다.
- 역전파 알고리즘은 신경망의 출력 오차를 출력층에서부터 역순으로 전파시켜 은닉층의 오차를 정의한 다음, 델타 규칙에 의해 각 계층의 가중치를 갱신합니다. 즉 역전파 알고리즘의 의의는 은닉 노드들의 오차를 정의하는 체계적인 방법을 제시한 데 있습니다.
- 단층 신경망은 선형 분리 가능한 문제만 다룰 수 있습니다. 그런데 대부분의 실제 문제는 선형 분리 가능하지 않습니다.
- 다층 신경망은 선형 분리 불가능한 문제도 모델링 가능합니다.
- 역전파 알고리즘에서 가중치 갱신식은 다양한 형태가 존재합니다. 다양한 가중치 갱신식의 변형이 개발된 이유는 신경망의 학습 안정성과 속도를 높여 학습이 잘 되게 하기 위해서입니다. 특히 학습시키기 어려운 딥러닝에서 이러한 특성은 요긴하게 쓰입니다.
- 비용함수는 신경망의 출력 오차를 계산하는 함수로 오차에 비례하는 값을 갖습니다. 최근에는 오차에 민감한 Cross entropy 함수가 많이 쓰입니다. 대개 Cross entropy 함수로부터 유도된 학습 규칙의 성능이 더 좋다고 알려져 있습니다.
- 신경망의 학습 규칙은 비용함수와 활성함수에 따라 조금씩 달라집니다. 특히 출력 노드에서 오차로부터 델타를 구하는 수식이 바뀝니다.
- 과적합 문제를 해결하는 기법 중의 하나인 정칙화 또한 비용함수에 가중치의 크기를 나타내는 항을 추가하는 방식으로 구현됩니다. 
  
## 4장 신경망과 분류

- 신경망으로 분류 모델을 만들 때는 이진 분류(범주 2개)인지 다범주 분류(범주 3개 이상)인지에 따라 출력층 노드의 개수와 활성함수를 다르게 구성하는게 보통입니다.
- 이진 분류 문제일 경우에는 출력 노드는 1개로 구성하고 활성함수로는 대개 시그모이드 함수를 사용합니다. 학습 데이터의 정답은 활성함수의 최대값과 최소값으로 변환합니다. 학습 규칙의 비용함수로는 Cross entropy 함수를 사용합니다.
- 다범주 분류 문제에서는 출력 노드는 범주의 개수만큼 구성합니다. 출력 노드의 활성함수로는 소프트맥스 함수를 사용합니다. 학습 데이터의 정답은 one-hot 인코딩을 통해 벡터로 변환합니다. 학습 규칙의 비용함수로는 Cross entropy 함수를 사용합니다.

## 5장 딥러닝

## 6장 컨벌루션 신경망