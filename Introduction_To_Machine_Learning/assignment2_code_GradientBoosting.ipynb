{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 책에서처럼 일단 공동 import를 하자\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# 불필요한 warning이 나오지 않도록 하는 곳\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient boosting을 사용\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784) (70000,)\n"
     ]
    }
   ],
   "source": [
    "# mnist 데이터셋을 불러오자\n",
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784')\n",
    "print(mnist.data.shape, mnist.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7000, 784)\n",
      "(63000, 784)\n",
      "(7000,)\n",
      "(63000,)\n"
     ]
    }
   ],
   "source": [
    "# train set과 test set을 분리해야 한다.\n",
    "# train과 test set을 그냥 numpy형태로 다 바꿔서 귀찮게 하지 말자.\n",
    "from sklearn.model_selection import train_test_split\n",
    "mnist_data = mnist.data.to_numpy()\n",
    "mnist_target = mnist.target.to_numpy()\n",
    "# training set을 default setting으로 해보자. 얘는 또 시간이 오래 걸리네......\n",
    "# test set accuracy를 높였지만 training 시간이 너무 올라간다.... 반반으로 나눠서 다시 테스트\n",
    "# 반반하고 default setting하고 일단 차이가 없네. 좀 더 줄여봐도 될 듯. training 시간은 그대로다. \n",
    "# training 시간을 줄이고 다른 변수들을 좀 더 조정하면서 overfitting은 피하고 test set의 accuracy를 올려야 한다.\n",
    "# 너무 오래 걸린다. 일단은 training set을 0.1로 줄여야 하겠다. logistic regression처럼\n",
    "X_train, X_test, y_train, y_test = train_test_split(mnist_data, mnist_target, train_size=0.1, random_state=0)\n",
    "# 잘 나뉘었는가 확인\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- default : learning_rate=.1, n_estimators=100, loss='log_loss'\n",
    "- 일단은 learning_rate를 .1로 고정하고 estimator를 50에서 300사이로 일단 경향을 확인해보자.\n",
    "- n_estimators를 500까지 올렸음에도 그에 비례해서 계속해서 test accuracy가 올라가는데..... 어느 선까지 확인이 가능할까.\n",
    "- 일단 700까지 테스트해보고 증가폭이 세번째 소수점 자리가 변화하지 않는거 같다 하면 멈추자.\n",
    "- 늘리면 늘릴수록 늘어나는데????? 한계를 모르는 것인가... 근데 300을 넘어가니까 training time이 너무 늘어난다. \n",
    "- n_estimator는 350으로 고정하고 learning_rate를 조정해가면서 체크해보자\n",
    "- 너무 느린데... learning rate의 영향력을 확인해보고 싶은 거니까 n=100으로 해놓고 다시 돌려보자. 그리고 jupyter notebook이 cpu를 온전히 다 활용하지 않는거 같은데 그 이유를 좀 더 찾아봐야 한다.\n",
    "- learning rate 0.35일때 일단은 가장 크더라\n",
    "- learning rate=0.35 n_estimator=550 train accuracy:1.0 / test accuracy:0.9431428571428572\n",
    "- 결론 : learning rate=0.35 n_estimator=550일때 train accuracy : 1.0, test accuracy : 0.943인 모델이 가장 높은걸로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate=0.1 n_estimator=50 train start\n",
      "learning rate=0.1 n_estimator=50 train end\n",
      "learning rate=0.1 n_estimator=100 train start\n",
      "learning rate=0.1 n_estimator=100 train end\n",
      "learning rate=0.1 n_estimator=150 train start\n",
      "learning rate=0.1 n_estimator=150 train end\n",
      "learning rate=0.1 n_estimator=200 train start\n",
      "learning rate=0.1 n_estimator=200 train end\n",
      "learning rate=0.1 n_estimator=250 train start\n",
      "learning rate=0.1 n_estimator=250 train end\n",
      "learning rate=0.1 n_estimator=300 train start\n",
      "learning rate=0.1 n_estimator=300 train end\n"
     ]
    }
   ],
   "source": [
    "# gradiend boosting 모델을 training data를 이용해 만들자\n",
    "\n",
    "gradboost_models = []\n",
    "train_score = []\n",
    "test_score = []\n",
    "l_rate_01 = .1\n",
    "n_estimators_list = [50,100,150,200,250,300]\n",
    "for n in n_estimators_list:\n",
    "    print(\"learning rate={} n_estimator={} train start\".format(l_rate, n))\n",
    "    gradboost = GradientBoostingClassifier(learning_rate=l_rate, n_estimators=n).fit(X_train, y_train)\n",
    "    train_score.append(gradboost.score(X_train, y_train))\n",
    "    test_score.append(gradboost.score(X_test, y_test))\n",
    "    gradboost_models.append(gradboost)\n",
    "    print(\"learning rate={} n_estimator={} train end\".format(l_rate_01, n))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate=0.1 n_estimator=50 train accuracy:0.9697142857142858 / test accuracy:0.9033015873015873\n",
      "learning rate=0.1 n_estimator=100 train accuracy:0.9945714285714286 / test accuracy:0.9213015873015873\n",
      "learning rate=0.1 n_estimator=150 train accuracy:0.9998571428571429 / test accuracy:0.9278571428571428\n",
      "learning rate=0.1 n_estimator=200 train accuracy:1.0 / test accuracy:0.932063492063492\n",
      "learning rate=0.1 n_estimator=250 train accuracy:1.0 / test accuracy:0.9349206349206349\n",
      "learning rate=0.1 n_estimator=300 train accuracy:1.0 / test accuracy:0.9368571428571428\n"
     ]
    }
   ],
   "source": [
    "# 예측을 잘 하는가?\n",
    "for i, n in enumerate(n_estimators_list):\n",
    "    print(\"learning rate={} n_estimator={} train accuracy:{} / test accuracy:{}\".format(l_rate, n, train_score[i], test_score[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate=0.1 n_estimator=300 train start\n",
      "learning rate=0.1 n_estimator=300 train end\n",
      "learning rate=0.1 n_estimator=350 train start\n",
      "learning rate=0.1 n_estimator=350 train end\n",
      "learning rate=0.1 n_estimator=400 train start\n",
      "learning rate=0.1 n_estimator=400 train end\n",
      "learning rate=0.1 n_estimator=450 train start\n",
      "learning rate=0.1 n_estimator=450 train end\n",
      "learning rate=0.1 n_estimator=500 train start\n",
      "learning rate=0.1 n_estimator=500 train end\n"
     ]
    }
   ],
   "source": [
    "# n_estimator를 300에서 500으로 늘려서 다시 한번 테스트\n",
    "\n",
    "gradboost_models_02 = []\n",
    "train_score_02 = []\n",
    "test_score_02 = []\n",
    "l_rate_02 = .1\n",
    "n_estimators_list_02 = [300,350,400,450,500]\n",
    "for n in n_estimators_list_02:\n",
    "    print(\"learning rate={} n_estimator={} train start\".format(l_rate, n))\n",
    "    gradboost = GradientBoostingClassifier(learning_rate=l_rate_02, n_estimators=n).fit(X_train, y_train)\n",
    "    train_score_02.append(gradboost.score(X_train, y_train))\n",
    "    test_score_02.append(gradboost.score(X_test, y_test))\n",
    "    gradboost_models_02.append(gradboost)\n",
    "    print(\"learning rate={} n_estimator={} train end\".format(l_rate_02, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate=0.1 n_estimator=300 train accuracy:1.0 / test accuracy:0.936968253968254\n",
      "learning rate=0.1 n_estimator=350 train accuracy:1.0 / test accuracy:0.9384285714285714\n",
      "learning rate=0.1 n_estimator=400 train accuracy:1.0 / test accuracy:0.9393809523809524\n",
      "learning rate=0.1 n_estimator=450 train accuracy:1.0 / test accuracy:0.9401428571428572\n",
      "learning rate=0.1 n_estimator=500 train accuracy:1.0 / test accuracy:0.9406984126984127\n"
     ]
    }
   ],
   "source": [
    "# 예측을 잘 하는가?\n",
    "for i, n in enumerate(n_estimators_list_02):\n",
    "    print(\"learning rate={} n_estimator={} train accuracy:{} / test accuracy:{}\".format(l_rate_02, n, train_score_02[i], test_score_02[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate=0.1 n_estimator=500 train start\n",
      "learning rate=0.1 n_estimator=500 train end\n",
      "learning rate=0.1 n_estimator=550 train start\n",
      "learning rate=0.1 n_estimator=550 train end\n",
      "learning rate=0.1 n_estimator=600 train start\n",
      "learning rate=0.1 n_estimator=600 train end\n",
      "learning rate=0.1 n_estimator=650 train start\n",
      "learning rate=0.1 n_estimator=650 train end\n",
      "learning rate=0.1 n_estimator=700 train start\n",
      "learning rate=0.1 n_estimator=700 train end\n"
     ]
    }
   ],
   "source": [
    "# n_estimator를 500에서 700으로 늘려서 다시 한번 테스트\n",
    "\n",
    "gradboost_models_03 = []\n",
    "train_score_03 = []\n",
    "test_score_03 = []\n",
    "l_rate_03 = .1\n",
    "n_estimators_list_03 = [500,550,600,650,700]\n",
    "for n in n_estimators_list_03:\n",
    "    print(\"learning rate={} n_estimator={} train start\".format(l_rate, n))\n",
    "    gradboost = GradientBoostingClassifier(learning_rate=l_rate_03, n_estimators=n).fit(X_train, y_train)\n",
    "    train_score_03.append(gradboost.score(X_train, y_train))\n",
    "    test_score_03.append(gradboost.score(X_test, y_test))\n",
    "    gradboost_models_03.append(gradboost)\n",
    "    print(\"learning rate={} n_estimator={} train end\".format(l_rate_03, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate=0.1 n_estimator=500 train accuracy:1.0 / test accuracy:0.9408571428571428\n",
      "learning rate=0.1 n_estimator=550 train accuracy:1.0 / test accuracy:0.9414285714285714\n",
      "learning rate=0.1 n_estimator=600 train accuracy:1.0 / test accuracy:0.9419206349206349\n",
      "learning rate=0.1 n_estimator=650 train accuracy:1.0 / test accuracy:0.9424761904761905\n",
      "learning rate=0.1 n_estimator=700 train accuracy:1.0 / test accuracy:0.9427142857142857\n"
     ]
    }
   ],
   "source": [
    "# 예측을 잘 하는가?\n",
    "for i, n in enumerate(n_estimators_list_03):\n",
    "    print(\"learning rate={} n_estimator={} train accuracy:{} / test accuracy:{}\".format(l_rate_03, n, train_score_03[i], test_score_03[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate=0.1 n_estimator=100 train start\n",
      "learning rate=0.1 n_estimator=100 train end\n",
      "learning rate=0.2 n_estimator=100 train start\n",
      "learning rate=0.2 n_estimator=100 train end\n",
      "learning rate=0.30000000000000004 n_estimator=100 train start\n",
      "learning rate=0.30000000000000004 n_estimator=100 train end\n",
      "learning rate=0.4 n_estimator=100 train start\n",
      "learning rate=0.4 n_estimator=100 train end\n",
      "learning rate=0.5 n_estimator=100 train start\n",
      "learning rate=0.5 n_estimator=100 train end\n",
      "learning rate=0.6000000000000001 n_estimator=100 train start\n",
      "learning rate=0.6000000000000001 n_estimator=100 train end\n",
      "learning rate=0.7000000000000001 n_estimator=100 train start\n",
      "learning rate=0.7000000000000001 n_estimator=100 train end\n",
      "learning rate=0.8 n_estimator=100 train start\n",
      "learning rate=0.8 n_estimator=100 train end\n",
      "learning rate=0.9 n_estimator=100 train start\n",
      "learning rate=0.9 n_estimator=100 train end\n"
     ]
    }
   ],
   "source": [
    "# n_estimator는 500으로 고정하고 learning rate를 조정해가면서 그 결과를 확인해보자.\n",
    "\n",
    "# 속도를 늘려줄까 과연?\n",
    "import joblib\n",
    "joblib.parallel_backend('threading', n_jobs=-1)\n",
    "\n",
    "gradboost_models_04 = []\n",
    "train_score_04 = []\n",
    "test_score_04 = []\n",
    "l_rate_list_04 = [0.1*i for i in range(1,10)]\n",
    "n_estimators_04 = 100\n",
    "for l_rate in l_rate_list_04:\n",
    "    print(\"learning rate={} n_estimator={} train start\".format(l_rate, n_estimators_04))\n",
    "    gradboost = GradientBoostingClassifier(learning_rate=l_rate, n_estimators=n_estimators_04).fit(X_train, y_train)\n",
    "    train_score_04.append(gradboost.score(X_train, y_train))\n",
    "    test_score_04.append(gradboost.score(X_test, y_test))\n",
    "    gradboost_models_04.append(gradboost)\n",
    "    print(\"learning rate={} n_estimator={} train end\".format(l_rate, n_estimators_04))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate=0.1 n_estimator=100 train accuracy:0.9945714285714286 / test accuracy:0.9212698412698412\n",
      "learning rate=0.2 n_estimator=100 train accuracy:1.0 / test accuracy:0.9286031746031747\n",
      "learning rate=0.30000000000000004 n_estimator=100 train accuracy:1.0 / test accuracy:0.930047619047619\n",
      "learning rate=0.4 n_estimator=100 train accuracy:1.0 / test accuracy:0.9284603174603174\n",
      "learning rate=0.5 n_estimator=100 train accuracy:1.0 / test accuracy:0.9256507936507936\n",
      "learning rate=0.6000000000000001 n_estimator=100 train accuracy:0.9992857142857143 / test accuracy:0.9077301587301587\n",
      "learning rate=0.7000000000000001 n_estimator=100 train accuracy:0.9997142857142857 / test accuracy:0.9041746031746032\n",
      "learning rate=0.8 n_estimator=100 train accuracy:0.9792857142857143 / test accuracy:0.8603492063492063\n",
      "learning rate=0.9 n_estimator=100 train accuracy:0.9787142857142858 / test accuracy:0.8502380952380952\n"
     ]
    }
   ],
   "source": [
    "# 예측을 잘 하는가?\n",
    "for i, l_rate in enumerate(l_rate_list_04):\n",
    "    print(\"learning rate={} n_estimator={} train accuracy:{} / test accuracy:{}\".format(l_rate, n_estimators_04, train_score_04[i], test_score_04[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate=0.225 n_estimator=100 train start\n",
      "learning rate=0.225 n_estimator=100 train end\n",
      "learning rate=0.25 n_estimator=100 train start\n",
      "learning rate=0.25 n_estimator=100 train end\n",
      "learning rate=0.275 n_estimator=100 train start\n",
      "learning rate=0.275 n_estimator=100 train end\n",
      "learning rate=0.325 n_estimator=100 train start\n",
      "learning rate=0.325 n_estimator=100 train end\n",
      "learning rate=0.35 n_estimator=100 train start\n",
      "learning rate=0.35 n_estimator=100 train end\n",
      "learning rate=0.375 n_estimator=100 train start\n",
      "learning rate=0.375 n_estimator=100 train end\n"
     ]
    }
   ],
   "source": [
    "# learning rate 0.2에서 0.4 사이를 0.025 간격으로 더해서 test해보자\n",
    "joblib.parallel_backend('threading', n_jobs=-1)\n",
    "\n",
    "gradboost_models_05 = []\n",
    "train_score_05 = []\n",
    "test_score_05 = []\n",
    "l_rate_list_05 = [0.225, 0.25, 0.275, 0.325, 0.35, 0.375]\n",
    "n_estimators_05 = 100\n",
    "for l_rate in l_rate_list_05:\n",
    "    print(\"learning rate={} n_estimator={} train start\".format(l_rate, n_estimators_05))\n",
    "    gradboost = GradientBoostingClassifier(learning_rate=l_rate, n_estimators=n_estimators_05).fit(X_train, y_train)\n",
    "    train_score_05.append(gradboost.score(X_train, y_train))\n",
    "    test_score_05.append(gradboost.score(X_test, y_test))\n",
    "    gradboost_models_05.append(gradboost)\n",
    "    print(\"learning rate={} n_estimator={} train end\".format(l_rate, n_estimators_05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate=0.225 n_estimator=100 train accuracy:1.0 / test accuracy:0.9297460317460318\n",
      "learning rate=0.25 n_estimator=100 train accuracy:1.0 / test accuracy:0.9307142857142857\n",
      "learning rate=0.275 n_estimator=100 train accuracy:1.0 / test accuracy:0.9305714285714286\n",
      "learning rate=0.325 n_estimator=100 train accuracy:1.0 / test accuracy:0.9301746031746032\n",
      "learning rate=0.35 n_estimator=100 train accuracy:1.0 / test accuracy:0.9314126984126984\n",
      "learning rate=0.375 n_estimator=100 train accuracy:1.0 / test accuracy:0.9287936507936508\n"
     ]
    }
   ],
   "source": [
    "# 예측을 잘 하는가?\n",
    "for i, l_rate in enumerate(l_rate_list_05):\n",
    "    print(\"learning rate={} n_estimator={} train accuracy:{} / test accuracy:{}\".format(l_rate, n_estimators_05, train_score_05[i], test_score_05[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate=0.35 n_estimator=500 train start\n",
      "learning rate=0.35 n_estimator=500 train end\n",
      "learning rate=0.35 n_estimator=550 train start\n",
      "learning rate=0.35 n_estimator=550 train end\n",
      "learning rate=0.35 n_estimator=600 train start\n",
      "learning rate=0.35 n_estimator=600 train end\n",
      "learning rate=0.35 n_estimator=650 train start\n",
      "learning rate=0.35 n_estimator=650 train end\n",
      "learning rate=0.35 n_estimator=700 train start\n",
      "learning rate=0.35 n_estimator=700 train end\n"
     ]
    }
   ],
   "source": [
    "# n_estimator를 500에서 700으 사이로 설정한 다음 l_rate를 .35로 바꿔서 학습시켜보자 다시.\n",
    "\n",
    "gradboost_models_06 = []\n",
    "train_score_06 = []\n",
    "test_score_06 = []\n",
    "l_rate_06 = .35\n",
    "n_estimators_list_06 = [500,550,600,650,700]\n",
    "for n in n_estimators_list_06:\n",
    "    print(\"learning rate={} n_estimator={} train start\".format(l_rate_06, n))\n",
    "    gradboost = GradientBoostingClassifier(learning_rate=l_rate_06, n_estimators=n).fit(X_train, y_train)\n",
    "    train_score_06.append(gradboost.score(X_train, y_train))\n",
    "    test_score_06.append(gradboost.score(X_test, y_test))\n",
    "    gradboost_models_06.append(gradboost)\n",
    "    print(\"learning rate={} n_estimator={} train end\".format(l_rate_06, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate=0.35 n_estimator=500 train accuracy:1.0 / test accuracy:0.9431269841269841\n",
      "learning rate=0.35 n_estimator=550 train accuracy:1.0 / test accuracy:0.9431428571428572\n",
      "learning rate=0.35 n_estimator=600 train accuracy:1.0 / test accuracy:0.943015873015873\n",
      "learning rate=0.35 n_estimator=650 train accuracy:1.0 / test accuracy:0.9432063492063492\n",
      "learning rate=0.35 n_estimator=700 train accuracy:1.0 / test accuracy:0.9431746031746032\n"
     ]
    }
   ],
   "source": [
    "# 예측을 잘 하는가?\n",
    "for i, n in enumerate(n_estimators_list_06):\n",
    "    print(\"learning rate={} n_estimator={} train accuracy:{} / test accuracy:{}\".format(l_rate_06, n, train_score_06[i], test_score_06[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction 1 : 5\n",
      "prediction 2 : 2\n",
      "prediction 3 : 3\n",
      "prediction 4 : 5\n",
      "prediction 5 : 8\n",
      "prediction 6 : 3\n",
      "prediction 7 : 9\n",
      "prediction 8 : 8\n",
      "prediction 9 : 9\n",
      "prediction 10 : 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test set에서 랜덤하게 10개를 뽑은 후 이를 출력해서 예측값과 실제 값이 맞는지 틀린지 확인해보자\n",
    "rand_index = np.random.randint(low=0, high=len(X_test), size=10)\n",
    "for randin, i in zip(rand_index, range(1,len(rand_index)+1)):\n",
    "    predict = X_test[randin].reshape(1,-1)\n",
    "    print(\"prediction {} : {}\".format(i,gradboost.predict(predict)[0]))\n",
    "    plt.subplot(2,5,i)\n",
    "    plt.imshow((X_test[randin].reshape(28,28)), cmap=plt.cm.gray_r, interpolation='nearest')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
